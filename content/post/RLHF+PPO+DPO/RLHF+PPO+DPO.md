---
title: "RLHF/PPO/DPO介绍"
date: 2024-10-17T19:20:40+08:00
draft: false 
params: 
  author: Lynn
tags: ["强化学习", "大模型对齐","LLM","算法"]
categories: ["论文阅读"]
description: "大模型对齐的RLHF/PPO/DPO技术介绍"
summary: "大模型对齐的RLHF/PPO/DPO技术介绍"
image:  "/image-20241017165214624.png"
typora-root-url: ./..\..\..\static

---

## 1、RLHF/PPO/DPO是什么？

RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）是一种机器学习（ML）技术，它利用人类反馈来优化 ML 模型，从而更有效地进行自我学习。

> 即：收集符合人类价值偏好的数据集，对LLM进行微调，使其回答向人类价值观对齐。

![img](/v2-ee215bedf45de4a5e08e5a3d04e2d275_1440w.webp)

[《Learning to summarize from human feedback》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2009.01325)

![image-20241017165214624](/image-20241017165214624.png)

PPO（Proximal Policy Optimization）是OpenAI在2017提出的一种强化学习算法，是基于策略优化的算法，用于训练能够最大化累积奖励的智能体。PPO算法通过在每次更新时限制新策略与旧策略之间的差异，从而更稳定地更新策略参数。这种方法有助于避免训练过程中出现的不稳定性和剧烈波动，使得算法更容易收敛并学习到更好的策略。

> 即：PPO是RLHF实施用到的具体算法

[《Proximal Policy Optimization Algorithms》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.06347)



DPO（Direct Preference Optimization， 直接偏好优化）是一种稳定的、性能和计算成本轻量级的强化学习算法。通过利用奖励函数与最优策略之间的映射关系，证明这个受限的奖励最大化问题可以通过单阶段的策略训练来精确优化，本质上是在人类偏好数据上解决一个分类问题。

> 即：DPO是相对于PPO更加稳定、低成本的强化学习方法

[《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.18290)

![image-20241017192604108](/image-20241017192604108.png)



## 2、为什么要使用RLHF/PPO/DPO？

LLM在预训练时是直接输入大量文本进行训练，根据当前token预测下一个token。那么当用户输入一段英文时，是继续编写下文 or 进行翻译 or 进行对话？，模型本身无法区分具体任务。因此需要指令微调，针对指定任务，可以针对性的优化LLM的性能。大部分LLM从功能上说到这里就足够了。

**但是对于一个成熟的LLM产品，不但功能要强，还不能胡编乱造，不能回答一些非法问题，不能触犯zzzq，因此RLHF就出现了。**

RLHF通过收集人类反馈数据（符合人类偏好的数据），训练出奖励模型，通过奖励模型评估LLM的答案，更新LLM权重，最终得到答案符合人类偏好的LLM。具体使用的策略就是PPO算法。

但是RLHF是一个复杂且经常不稳定的过程，首先拟合反映人类偏好的奖励模型，然后使用强化学习微调大型无监督 LM，以最大化这种估计奖励，而不会偏离原始模型太远。且PPO需要收集大量人类偏好数据、需要训练奖励模型、RLHF需要同时加载多个模型进行训练，训练难度较大。

提出了使用DPO的方式进行训练。

![img](/v2-1502534b7e614e64b3a9e1a4568ee93a_r.jpg)




## 3. PPO（**Proximal Policy Optimization**，**近端策略优化**）

PPO（**Proximal Policy Optimization**，**近端策略优化**）是一种用于强化学习的算法，主要目的是在训练过程中保证策略更新的**稳定性**和**效率**。

简单来说，PPO通过限制每次策略更新的幅度，避免模型在学习过程中发生大幅度的性能波动，从而使训练更加稳定。它主要用于**优化策略**，即帮助智能体根据环境中的反馈（奖励）来不断改进它的行动策略，使其能够更好地完成任务。

### PPO的关键特点：
1. **策略更新的限制**：每次更新时，PPO会确保新的策略不会偏离旧策略太远，防止策略更新过大带来的不稳定性。
2. **计算高效**：PPO相比其他复杂的强化学习算法（如TRPO）更简单，同时还能保证较好的效果，因而在大语言模型的强化学习（如RLHF）中被广泛应用。

### PPO的具体算法：
**PPO（Proximal Policy Optimization）**通过引入**剪切（clipping）**和**信赖域约束**来限制每次策略更新的幅度，确保模型在训练过程中保持稳定性。下面是PPO如何具体限制更新幅度的详细解释：

#### 1. 策略比值（Ratio of Policies）

PPO基于强化学习中的策略梯度方法，目的是通过迭代优化策略，使得智能体能够更好地根据环境作出决策。在每次更新时，PPO引入了一个**策略比值（ratio of policies）**，用来表示新策略与旧策略在同一状态下选择相同行动的概率比：

$$
r(\theta) = \frac{\pi_{\theta_{\text{new}}}(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}
$$

其中：
- $\pi_{\theta_{\text{new}}}(a|s)$ 是当前新策略（参数为$\theta_{\text{new}}$）在状态 $s$ 下选择动作 $a$ 的概率。
- $\pi_{\theta_{\text{old}}}(a|s)$ 是旧策略（参数为$\theta_{\text{old}}$）在同一状态下选择动作 $a$ 的概率。

这个比值 $r(\theta)$ 反映了新旧策略在当前状态和动作下的差异。

#### 2. 限制策略更新的幅度：剪切（Clipping）机制

PPO的核心思想是防止新策略与旧策略差异过大，因此它使用**剪切（clipping）机制**来限制策略更新的幅度。具体做法是：

$$
L^{\text{CLIP}}(\theta) = \min \left( r(\theta) \cdot \hat{A}(s,a), \, \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) \cdot \hat{A}(s,a) \right)
$$

其中：
- $ \hat{A}(s, a) $ 是**优势函数（advantage function）**，表示在给定状态 $s$ 下选择动作 $a $的优势（即相对于其他可能的动作，这个动作有多好）。
- $\epsilon$ 是一个超参数，通常取值较小（例如 0.1 或 0.2），用于控制策略更新的幅度。

PPO通过这个公式的**最小值操作**，确保策略更新时的比值 $r(\theta)$$ 不会偏离 1 太多。如果 $$r(\theta)$$ 的变化超过了 $$1 + \epsilon$$ 或小于 $$1 - \epsilon$，更新就会被限制在 $[1 - \epsilon, 1 + \epsilon]$ 之间。这种限制能够防止策略的更新过大，避免强化学习过程中出现不稳定的情况。

**简化解释：**

- 如果新策略和旧策略的差异太大（即 $r(\theta)$ 过大或过小），模型就会限制更新幅度，不允许它过度偏离原来的策略。
- 剪切机制有效地防止了过大更新，从而使策略的每次改进更加平稳。

#### 3. 信赖域约束（Trust Region Constraint）与 PPO 的差异

在 PPO 之前，有一种更复杂的策略优化算法叫做 **TRPO（Trust Region Policy Optimization）**，其通过引入信赖域约束来防止策略更新过大。TRPO 通过一个较为复杂的优化过程来确保新旧策略之间的“距离”（通常以 KL散度度量）不会超过某个阈值。

相比之下，PPO的**剪切机制**是一种更简单的方式实现类似的目标，直接通过限制策略比值 $r(\theta)$ 的变化范围来控制更新的幅度。这种方式虽然相对简单，但效果显著，且计算效率更高，因此被广泛采用。

#### 4. PPO优化目标

最终的优化目标是通过最大化“裁剪后的目标函数”（即上面提到的 $\min$ 操作结果）来更新策略。这个目标函数鼓励策略在优势较大的状态下进行更新，但同时避免了策略变化过大可能带来的不稳定性。

#### 5. 总结

**PPO如何限制更新幅度**：
- **策略比值**：通过计算新旧策略在相同行动上的概率比，衡量策略的变化。
- **剪切机制**：如果策略比值超出了允许范围（通常设为 $[1 - \epsilon, 1 + \epsilon]$），则剪切比值，限制策略更新幅度。
- **效果**：这种方法使得策略每次更新时不会变化太大，保持了强化学习过程的稳定性，并且相比更复杂的 TRPO，PPO 的效率更高，适用于大规模模型训练。

因此，PPO 是一种平衡了策略改进速度与训练稳定性的强化学习算法，通过限制策略的更新幅度，确保每次训练迭代都在“信赖域”内进行，避免策略偏离太远导致不稳定的情况。




## DPO（Direct Preference Optimization，直接偏好优化）

**DPO（Direct Preference Optimization，直接偏好优化）** 是一种新兴的用于对齐语言模型的技术。与传统的强化学习方法（如RLHF）不同，DPO不依赖于奖励模型或强化学习的复杂机制，而是通过直接利用人类偏好数据来优化模型，使模型生成的输出更加符合人类的期望。

### 1. DPO 的基本概念

在 **RLHF（Reinforcement Learning from Human Feedback）** 中，首先需要用人类反馈训练一个奖励模型，之后通过强化学习来优化语言模型的策略。相比之下，**DPO** 跳过了这个复杂的过程，直接基于人类偏好对模型进行优化，简化了训练步骤并减少了计算成本。

### 2. DPO 的核心机制

DPO 的核心思想是通过直接偏好反馈来训练模型。其步骤如下：

#### 1. 收集偏好数据

在 DPO 中，首先需要从人类标注员那里收集偏好数据。这些数据通过让人类比较多个模型生成的输出，选择他们认为“更好”的结果，构建成“偏好对”（preference pairs）。每一组偏好数据包含：
- **输入**（例如一个问题或任务指令）
- **多个模型生成的响应**（这些响应基于输入生成）
- **人类的偏好选择**（标注员选择认为更好的响应）

例如，如果模型生成了两个输出，标注员会选择其中一个作为更优结果，形成一个二元偏好对。

#### 2. 构建优化目标：偏好概率模型

DPO 直接利用人类的偏好反馈来优化模型。它通过引入一个**偏好概率模型**，该模型通过比较两个输出生成的概率，来优化更符合人类偏好的输出。具体来说，给定两个模型输出的偏好对，DPO 通过最大化人类选择的“更好”响应的生成概率来训练模型。

假设我们有两个输出：
- **输出1**（人类偏好）：$y_1$
- **输出2**（人类不偏好）：$y_2$

对于这两个输出，DPO 使用以下概率模型来表示人类选择某个输出的概率：

$$ P(y_1 \,|\, x) = \frac{\exp(\pi_\theta(x, y_1))}{\exp(\pi_\theta(x, y_1)) + \exp(\pi_\theta(x, y_2))} $$

这里：
- $x$ 是输入，$y_1$ 和 $y_2$ 是模型生成的两个响应。
- $\pi_\theta(x, y)$ 是模型的生成得分函数（可以理解为模型对特定输出的信心得分，或称为“对数概率”）。
- 这个概率模型表示人类选择 $y_1$ 的概率与它的生成得分成正比。

#### 3. 损失函数：最大化正确偏好概率

DPO 的优化目标是通过最大化人类偏好的选择概率来调整模型参数。对于每一个偏好对，DPO 希望模型能够生成让人类偏好的那个输出 $y_1$ 的概率最大。因此，DPO 构建的损失函数为：

$$ L(\theta) = -\log P(y_1 \,|\, x) $$

该损失函数会使得在偏好数据中被人类选择的输出（$y_1$）的生成概率不断增大，最终模型会倾向于生成更符合人类偏好的输出。

#### 4. 训练过程

DPO 使用偏好对数据进行训练的具体步骤如下：
1. **输入采样**：给定一组输入，模型会生成多个可能的输出（即候选响应）。
2. **人类偏好选择**：人类标注员会从这些输出中选择更好的一个作为优选输出。
3. **构建损失**：根据人类的选择，构建损失函数。模型更新的目标是最大化人类偏好的响应生成概率，同时最小化不被偏好响应的生成概率。
4. **参数更新**：使用反向传播算法，基于偏好对数据调整模型参数，使得模型在未来能够生成更符合人类偏好的结果。

### 3. 优势函数的消除

与 RLHF 不同的是，DPO 并不需要通过奖励模型计算优势函数（advantage function）。在 RLHF 中，优势函数用于评估当前策略输出的相对优越性，而 DPO 直接使用偏好对中的二元选择进行优化，减少了对中间步骤的依赖。

### 4. 相对偏好学习

DPO 实质上是一种**相对偏好学习**。它并不要求绝对的“正确”答案，而是只需要模型根据偏好对中较为优选的结果进行优化。这种相对偏好学习方式降低了对标注数据的要求，因为它不需要明确的标签，而只需要“更好”或“更差”的相对比较。

### 5. 与对比学习的关系

DPO 在一定程度上类似于**对比学习**（Contrastive Learning），因为它依赖于比较多个候选输出，并通过最大化正确响应相对其他响应的优势来训练模型。不同的是，DPO 不需要对比所有可能的候选输出，而是仅在一对候选输出中进行优化。

### 6. 总结：DPO 的工作流程

1. **数据收集**：人类标注员为同一输入选择两个候选输出，形成偏好对。
2. **概率建模**：通过构建选择优选响应的概率模型，计算两个候选响应的概率比。
3. **损失计算**：最大化人类偏好的响应的生成概率，构建损失函数。
4. **训练与更新**：通过损失函数优化模型参数，使得未来的输出更符合人类的偏好。

DPO 的核心机制在于直接从人类的偏好数据中学习，并通过概率建模和损失函数的设计，简化了对齐模型的训练流程。这使得 DPO 在计算上更加高效，适合于快速对齐任务的场景。

